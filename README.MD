# ğŸ“˜ **CENSYS â€” Sistema RAG Local por Documento**

---

# ğŸ–¥ï¸ **Requisitos mÃ­nimos del sistema**

Para ejecutar el sistema RAG de forma correcta, la PC debe cumplir:

### âœ” **Hardware mÃ­nimo**

* **Procesador:** Intel i5 / Ryzen 5 o superior
* **RAM:** 8 GB (mÃ­nimo)
* **Disco:** 2 GB libres para documentos + Ã­ndices + modelos
* **GPU:** opcional (Ollama puede usar CPU)

### âœ” **Hardware recomendado**

* **Procesador:** Intel i7 / Ryzen 7
* **RAM:** 16 GB (ideal para PDFs largos)
* **SSD:** recomendado para mayor velocidad en embeddings
* **GPU NVIDIA/AMD/Apple:** mejora notablemente Ollama (opcional)

### âœ” **Software**

* Windows, Linux o macOS
* Python **3.11+** el utilizado fue 3.11.9
* Node.js **18+**
* Ollama instalado
* Navegador moderno (Chrome/Edge/Firefox)

---

# ğŸ§  **CENSYS â€” Sistema RAG Local por Documento**

El RAG es un sistema de consulta inteligente que permite cargar documentos, procesarlos localmente y realizar preguntas con un chatbot usando **IA completamente offline** gracias a Ollama.

ğŸ‘‰ Cada computadora tiene **sus propios documentos e Ã­ndices**, nada se comparte automÃ¡ticamente.

---

# ğŸ§© TecnologÃ­as utilizadas

### ğŸ–¼ Frontend

* React + Vite
* Tailwind + ShadCN UI
* Lucide Icons
* **Figma Make**
  â†’ utilizado para diseÃ±ar todas las vistas y estructura visual rÃ¡pidamente.

### âš™ Backend

* FastAPI
* Python 3.11
* **Ollama** como motor LLM offline
* **ChatGPT** utilizado durante el desarrollo para:

  * decidir modelos (llama3, mistral, phi3, mxbai)
  * revisar errores en pipeline RAG
  * depurar chunking / embeddings / prompts
  * mejorar precisiÃ³n y evitar alucinaciones

### ğŸ§  IA / RAG

* ExtracciÃ³n automÃ¡tica de texto
* Limpieza y normalizaciÃ³n
* Chunking configurable
* Embeddings con `mxbai-embed-large`
* Similaridad coseno
* GeneraciÃ³n con `llama3:instruct`

---

# ğŸ—ï¸ **Arquitectura del sistema**

```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        FRONTEND          â”‚
               â”‚  React + Vite + UI       â”‚
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
               â”‚  Subir documentos        â”‚
               â”‚  Ver documentos          â”‚
               â”‚  Chat                    â”‚
               â”‚  Chat flotante           â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP (REST)
                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               BACKEND (FastAPI)          â”‚
        â”‚                                          â”‚
        â”‚  /upload     â†’ guardar + extraer + index â”‚
        â”‚  /documents  â†’ listar archivos           â”‚
        â”‚  /query      â†’ RAG + llamada a LLM       â”‚
        â”‚                                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     OLLAMA (LLM)       â”‚
              â”‚ llama3:instruct        â”‚
              â”‚ mxbai-embed-large      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

# ğŸ“‚ Estructura del proyecto

```
Back/
 â”œâ”€â”€ app/
 â”‚   â”œâ”€â”€ main.py
 â”‚   â”œâ”€â”€ rag.py
 â”‚   â”œâ”€â”€ storage.py
 â”‚   â”œâ”€â”€ text_extractor.py
 â”‚   â”œâ”€â”€ config.py
 â”‚   â””â”€â”€ schemas.py
 â”‚
 â”œâ”€â”€ data/
 â”‚   â”œâ”€â”€ docs/        â† documentos locales
 â”‚   â”œâ”€â”€ indexes/     â† embeddings + chunks locales
 â”‚
Front/
 â”œâ”€â”€ src/components/
 â”‚   â”œâ”€â”€ UploadPage.tsx
 â”‚   â”œâ”€â”€ DocumentsPage.tsx
 â”‚   â”œâ”€â”€ ChatPage.tsx
 â”‚   â””â”€â”€ FloatingChat.tsx
 â””â”€â”€ utils/api.ts
```

---

# ğŸ”¥ Funcionamiento del chunking

El chunking divide el documento en partes pequeÃ±as para:

* mejorar la precisiÃ³n
* evitar pÃ©rdida de contexto
* optimizar embeddings

CÃ³digo utilizado:

```python
def chunk_text(text: str, chunk_size=220, overlap=40):
    words = text.split()
    chunks = []
    
    i = 0
    while i < len(words):
        chunk = words[i:i + chunk_size]
        chunks.append(" ".join(chunk))
        i += chunk_size - overlap

    return chunks
```

### ExplicaciÃ³n:

* chunk_size = 220 palabras
* overlap = 40 palabras superpuestas
* evita cortes de concepto entre chunks
* mejora la bÃºsqueda semÃ¡ntica

### CuÃ¡ndo cambiar chunk_size

* documentos muy tÃ©cnicos â†’ chunk_size mayor
* documentos con definiciones cortas â†’ chunk_size menor
* modelos pequeÃ±os â†’ chunks pequeÃ±os

---

# ğŸ¤– CÃ³mo cambiar el modelo LLM

En:

```
Back/app/config.py
```

EstÃ¡ esta lÃ­nea:

```python
LLM_MODEL = "llama3:instruct"
```

Puedes cambiarla por cualquier modelo de Ollama:

### Modelos disponibles:

* `llama3:instruct` (recomendado)
* `llama3`
* `mistral:instruct`
* `phi3:instruct`
* `phi3:mini`
* `qwen2:7b-instruct`

Ejemplo:

```python
LLM_MODEL = "mistral:instruct"
```

Luego reiniciar backend:

```sh
uvicorn app.main:app --reload
```

---

# ğŸŒ CÃ³mo cambiar el idioma del chat

El idioma estÃ¡ controlado en:

```
rag.py â†’ answer() â†’ system_prompt
```

Actual:

```python
"Eres un asistente que responde SIEMPRE en espaÃ±ol..."
```

### Cambiar a inglÃ©s:

```python
"Eres un asistente que responde SIEMPRE en inglÃ©s..."
```

### Responder en el idioma de la pregunta:

```python
"Responde SIEMPRE en el mismo idioma que la pregunta..."
```

---

# ğŸš€ InstalaciÃ³n

### Backend

```sh
cd Back
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

### Frontend

```sh
cd Front
npm install
npm run dev
```

# ğŸ‰ ConclusiÃ³n

Censys es una plataforma RAG local, segura y rÃ¡pida, diseÃ±ada con:

* UI creada en Figma Make
* Backend optimizado con ayuda de ChatGPT
* Modelos Ollama completamente locales
* Flujo RAG robusto para PDFs largos
* Independencia total por computadora


